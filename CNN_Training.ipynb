{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip Fotos.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "carpeta_imagenes = \"./Gas-Meter-Counter/JPEGImages\"\n",
    "carpeta_anotaciones = \"./Gas-Meter-Counter/Annotations\"\n",
    "carpeta_anotaciones_digits = \"./Gas-Meter-Counter/Annotations_digits\"\n",
    "carpeta_salida = \"./Salida recortes\"\n",
    "carpeta_salida_entrenar = \"./Salida_para_entrenar\"\n",
    "\n",
    "#crop images\n",
    "import numpy as np # linear algebra\n",
    "import xml.etree.ElementTree as ET # for parsing XML\n",
    "from PIL import Image # to read images\n",
    "import os\n",
    "import glob\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import uuid\n",
    "\n",
    "root_images=carpeta_imagenes\n",
    "root_annots=carpeta_anotaciones\n",
    "root_annots_digits=carpeta_anotaciones_digits\n",
    "\n",
    "all_images=os.listdir(carpeta_imagenes)\n",
    "print(f\"Total images : {len(all_images)}\")\n",
    "\n",
    "breeds = glob.glob(carpeta_anotaciones)\n",
    "annotation=[]\n",
    "for b in breeds:\n",
    "    annotation+=glob.glob(b+\"/*\")\n",
    "print(f\"Total annotation : {len(annotation)}\")\n",
    "\n",
    "breeds = glob.glob(carpeta_anotaciones_digits)\n",
    "annotation_digits=[]\n",
    "for b in breeds:\n",
    "    annotation_digits+=glob.glob(b+\"/*\")\n",
    "print(f\"Total annotation_digits : {len(annotation_digits)}\")\n",
    "\n",
    "breed_map={}\n",
    "for annot in annotation_digits:\n",
    "    breed=annot.split(\"/\")[-2]\n",
    "    index=breed.split(\"-\")[0]\n",
    "    breed_map.setdefault(index,breed)\n",
    "\n",
    "\n",
    "\n",
    "def get_digits(image):\n",
    "    bpath=root_annots_digits+\"/\"+str(image.split(\".\")[0]+\".xml\")\n",
    "    try:\n",
    "      tree = ET.parse(bpath)\n",
    "    except:\n",
    "      return []\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall('object')\n",
    "    \n",
    "    digits = []\n",
    "    for o in objects:\n",
    "        name = o.find('name').text # reading bound box\n",
    "        digits.append(name)\n",
    "        \n",
    "    return digits\n",
    "\n",
    "def bounding_box(image):\n",
    "    bpath=root_annots+\"/\"+str(image.split(\".\")[0]+\".xml\")\n",
    "    tree = ET.parse(bpath)\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall('object')\n",
    "    \n",
    "    for o in objects:\n",
    "        bndbox = o.find('bndbox') # reading bound box\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        \n",
    "        \n",
    "    return (xmin,ymin,xmax,ymax)\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "bbox=[]\n",
    "for i,image in enumerate(all_images):\n",
    "    image = all_images[1]\n",
    "    bbox=bounding_box(image) \n",
    "    digits = get_digits(image)\n",
    "    im = cv.imread(os.path.join(root_images,image),cv.IMREAD_GRAYSCALE)\n",
    "    im = cv.equalizeHist(im)\n",
    "    \n",
    "    im = im[bbox[1]:bbox[3],bbox[0]:bbox[2]]\n",
    "    #cv.imshow(\"\",im)\n",
    "    for i in range(len(digits)):\n",
    "      num = digits[i]\n",
    "      \n",
    "      ancho = int(im.shape[1]/len(digits))\n",
    "      dig_im = im[0:im.shape[0],(i*ancho):(i*ancho)+ancho]\n",
    "      dim = (28, 28)\n",
    "      dig_im = cv.resize(dig_im, dim, interpolation = cv.INTER_AREA)\n",
    "      #cv.imshow(\"\",dig_im)\n",
    "      cv.imwrite(carpeta_salida_entrenar+'/{}_{}.jpg'.format(num,uuid.uuid4().hex),dig_im)\n",
    "print(\"Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import InputLayer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import random,os\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from numpy.random import seed\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def esPosible(entrada,filtro,num_filtros,padding,pool):\n",
    "    actual = entrada\n",
    "    for num in num_filtros:\n",
    "        salida = ((actual - filtro[0]) / pool[0])\n",
    "        if padding: salida = (actual / pool[0])\n",
    "        actual = math.ceil(salida)\n",
    "        if salida <= 0: \n",
    "            print(\"No se puede para \", entrada, filtro, num_filtros, padding)\n",
    "            return 0\n",
    "    return math.ceil(salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(activation,padding,dropout,balanceo,optimizador,filtro,num_filtro,pool,capasPM):\n",
    "    directorio = \"./Resultados/\"\n",
    "    semilla=1\n",
    "    valores = classes.values()\n",
    "    claves = classes.keys()\n",
    "    maximo = max(valores)\n",
    "    pesos_clases = []\n",
    "    for peso in valores:\n",
    "        pesos_clases.append(maximo/peso)\n",
    "    aux = []\n",
    "    for clave in claves:\n",
    "        aux.append(int(clave))\n",
    "    claves = aux\n",
    "    pesos_clases = dict(zip(claves,pesos_clases))\n",
    "    seed(semilla)\n",
    "    tf.random.set_seed(semilla)\n",
    "    cadena = \"ACTIVATION({})_PADDING({})_DROPOUT({})_BALANCEO({})_OPTIMIZADOR({})_FILTRO({})_FILTROS({})_POOL({})_CAPAS-PM({})\".format(activation,padding,dropout,balanceo,optimizador,filtro,num_filtro,pool,capasPM)\n",
    "    resultados = directorio + \"LossFig\"\n",
    "    hechos = os.listdir(resultados)\n",
    "    aux = cadena + \"loss.png\"\n",
    "    if aux in hechos:\n",
    "        print(\"Ya esta hecho\")\n",
    "        return 1\n",
    "    if esPosible(28,filtro,num_filtro,padding,pool) > 0:\n",
    "        salida = esPosible(28,filtro,num_filtro,padding,pool)\n",
    "        salida = salida * salida * num_filtro[-1]\n",
    "        #Esta es la parte de la CNN\n",
    "        model = Sequential()\n",
    "        model.add(BatchNormalization(input_shape=(28,28,1)))\n",
    "        for num in num_filtro:\n",
    "            if padding: model.add(Conv2D(num, filtro, padding='same', activation=activation))\n",
    "            else: model.add(Conv2D(num, filtro, activation=activation))\n",
    "            model.add(MaxPool2D(pool_size=pool))\n",
    "            if dropout > 0: model.add(Dropout(dropout))\n",
    "        model.add(Flatten())\n",
    "        #Esta es la parte del PM\n",
    "        for capa in range(capasPM):\n",
    "            model.add(Dense(salida,activation=activation))\n",
    "            salida = salida // 2\n",
    "            if dropout > 0: model.add(Dropout(dropout))\n",
    "        model.add(Dense(10, activation = \"softmax\"))\n",
    "        model.compile(optimizer=optimizador,loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "        model.summary(line_length=120)\n",
    "        if not balanceo:\n",
    "            historico = model.fit(x_train, y_train, \n",
    "                                  epochs=500, \n",
    "                                  validation_freq=1,\n",
    "                                  validation_data=(x_test, y_test))\n",
    "        else:\n",
    "            historico = model.fit(x_train, y_train, \n",
    "                                  epochs=500, \n",
    "                                  validation_freq=1,\n",
    "                                  validation_data=(x_test, y_test),\n",
    "                                  class_weight=pesos_clases)\n",
    "        ## plots de evolución de loss y accuracy\n",
    "        \n",
    "        from matplotlib import pyplot as plt\n",
    "        plt.plot(historico.history['accuracy'])\n",
    "        plt.plot(historico.history['val_accuracy'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.savefig(directorio + \"AccFig/\" + cadena + \"accuracy.png\")\n",
    "        plt.show()\n",
    "        plt.plot(historico.history['loss'])\n",
    "        plt.plot(historico.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.show()\n",
    "        plt.savefig(directorio + \"LossFig/\" + cadena + \"loss.png\")\n",
    "        np.savetxt(directorio + \"TrainLoss/\" + cadena + 'historicoTrainLoss.txt',historico.history['loss'])\n",
    "        np.savetxt(directorio + \"ValLoss/\" + cadena + 'historicoValLoss.txt',historico.history['val_loss'])\n",
    "        np.savetxt(directorio + \"TrainAcc/\" + cadena + 'historicoTrainAcc.txt',historico.history['accuracy'])\n",
    "        np.savetxt(directorio + \"ValAcc/\" + cadena + 'historicoValAcc.txt',historico.history['val_accuracy'])\n",
    "        model.save(directorio + \"Modelos/\" + cadena + 'modelo.h5')\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "carpeta_salida_entrenar = \"./Salida_para_entrenar\"\n",
    "all_images=os.listdir(carpeta_salida_entrenar)\n",
    "random.shuffle(all_images)\n",
    "\n",
    "_y = []\n",
    "for image in all_images:\n",
    "  _y.append(image.split(\"_\")[0])\n",
    "\n",
    "\n",
    "unique, counts = np.unique(_y, return_counts=True)\n",
    "classes = dict(zip(unique, counts))\n",
    "print(classes)\n",
    "\n",
    "mean = np.mean(counts)\n",
    "mean = int(mean + mean*0.2)\n",
    "\n",
    "for i,count in zip(unique,counts):\n",
    "  if count > mean:\n",
    "    for j in range(count - mean):\n",
    "      for image in all_images:\n",
    "        if i == image.split(\"_\")[0]:\n",
    "          all_images.remove(image)\n",
    "          break\n",
    "\n",
    "train_images = all_images[:int(len(all_images)*0.8)]\n",
    "test_images = all_images[int(len(all_images)*0.8):]\n",
    "x_train = np.ndarray(shape=(len(train_images),28,28,1))\n",
    "y_train = np.ndarray(shape=(len(train_images),))\n",
    "x_test = np.ndarray(shape=(len(test_images),28,28,1))\n",
    "y_test = np.ndarray(shape=(len(test_images),))\n",
    "for i in range(len(train_images)):\n",
    "  image = train_images[i]\n",
    "  data = cv.imread(os.path.join(carpeta_salida_entrenar,image),cv.IMREAD_GRAYSCALE)\n",
    "  x_train[i] = np.expand_dims(data, axis=-1)\n",
    "  y_train[i] = image.split(\"_\")[0]\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "  image = test_images[i]\n",
    "  data = cv.imread(os.path.join(carpeta_salida_entrenar,image),cv.IMREAD_GRAYSCALE)\n",
    "  x_test[i] = np.expand_dims(data, axis=-1)\n",
    "  y_test[i] = image.split(\"_\")[0]\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "entrada=28\n",
    "filtros = [(2,2), (3,3)]\n",
    "num_filtros = [[32,16],[32],[32,16,8]]\n",
    "padding = [True, False]\n",
    "pools = [(2,2), (3,3)]\n",
    "for fil in filtros:\n",
    "    for num in num_filtros:\n",
    "        for pad in padding:\n",
    "            for pool in pools:\n",
    "                esPosible(entrada,fil,num,pad,pool)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-plate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "inicio = time.time()\n",
    "activations = [\"relu\", \"sigmoid\", \"tanh\"]\n",
    "padding = [True, False]\n",
    "dropout = [0,0.25, 0.5, 0.75]\n",
    "balanceo = [True, False]\n",
    "optimizador = [\"adam\", \"adamax\", \"nadam\"]\n",
    "filtros = [(2,2), (3,3)]\n",
    "num_filtros = [[32,16],[32]]\n",
    "pools = [(2,2), (3,3)]\n",
    "capas_pms = [2,3] \n",
    "hechos = 0\n",
    "for act in activations:\n",
    "    for pad in padding:\n",
    "        for drop in dropout:\n",
    "            for bal in balanceo:\n",
    "                for opt in optimizador:\n",
    "                    for filtro in filtros:\n",
    "                        for num in num_filtros:\n",
    "                            for pool in pools:\n",
    "                                for capa in capas_pms:\n",
    "                                    hechos += train(act,pad,drop,bal,opt,filtro,num,pool,capa)\n",
    "                                    print(\"Hechos: \",hechos)\n",
    "fin = time.time()\n",
    "print(fin-inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50139bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cadena=\"\"\n",
    "model = load_model(cadena)\n",
    "evaluacion_cnn=model.evaluate(x_test, y_test)\n",
    "#Obtencion de predicciones de CNN \n",
    "#predicciones en bruto:\n",
    "raw_testPred_cnn = model.predict(x_test)\n",
    "#predicciones de la clase:\n",
    "class_testPred_cnn = np.argmax(raw_testPred_cnn, axis=1)\n",
    "#Matriz de confusion y Classification report de CNN\n",
    "cm_cnn=confusion_matrix(y_test, class_testPred_cnn)\n",
    "print(cm_cnn)\n",
    "sns.heatmap(cm_cnn,cmap=\"Greens\",annot=True,xticklabels=classes.keys(),yticklabels=classes.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Matriz de confusión\")\n",
    "plt.show()\n",
    "cm_normalized = np.round(cm_cnn/np.sum(cm_cnn,axis=1).reshape(-1,1),2)\n",
    "print(cm_normalized)\n",
    "sns.heatmap(cm_normalized,cmap=\"Greens\",annot=True,xticklabels=classes.keys(),yticklabels=classes.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Matriz de confusión normalizada\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r resultados.zip Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed668125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
